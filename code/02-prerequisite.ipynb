{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  基本数据操作"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 1.3.1\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(\"PyTorch version:\", torch.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 创建PyTorch Tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.1210e-44, 0.0000e+00, 0.0000e+00],\n",
      "        [0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
      "        [0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
      "        [0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
      "        [0.0000e+00, 0.0000e+00, 0.0000e+00]])\n"
     ]
    }
   ],
   "source": [
    "x = torch.empty(5, 3)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.7753, 0.5968, 0.3043],\n",
      "        [0.8182, 0.9059, 0.0084],\n",
      "        [0.0928, 0.0080, 0.9260],\n",
      "        [0.4166, 0.8482, 0.8696],\n",
      "        [0.0504, 0.7147, 0.5023]])\n"
     ]
    }
   ],
   "source": [
    "x = torch.rand(5, 3)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "指定数据类型："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0, 0, 0],\n",
      "        [0, 0, 0],\n",
      "        [0, 0, 0],\n",
      "        [0, 0, 0],\n",
      "        [0, 0, 0]])\n",
      "torch.int64\n"
     ]
    }
   ],
   "source": [
    "x = torch.zeros(5, 3, dtype=torch.long)\n",
    "print(x)\n",
    "print(x.dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "从现有array创建："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([5.5000, 3.0000])\n",
      "tensor([[5.2000, 2.0000],\n",
      "        [1.2000, 9.0000]])\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor([5.5, 3])\n",
    "print(x)\n",
    "y = torch.tensor([[5.2, 2], [1.2, 9]])\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "从现有的tensor创建："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.],\n",
      "        [1., 1., 1.],\n",
      "        [1., 1., 1.],\n",
      "        [1., 1., 1.]])\n",
      "tensor([[-0.3641,  0.3783,  0.4136],\n",
      "        [ 0.0720, -1.4639,  1.2516],\n",
      "        [ 1.1459, -0.7147, -0.0098],\n",
      "        [ 0.7281, -0.3607, -0.6848],\n",
      "        [-0.8404,  1.4801,  0.6986]])\n"
     ]
    }
   ],
   "source": [
    "x = x.new_ones(5, 3)\n",
    "print(x)\n",
    "x = torch.randn_like(x, dtype=torch.float)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "获取tensor形状："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 3])\n",
      "torch.Size([5, 3])\n"
     ]
    }
   ],
   "source": [
    "print(x.shape)\n",
    "print(x.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 PyTorch Tensor操作"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "加法："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.3062,  1.0174,  0.9678],\n",
      "        [ 0.9446, -0.9590,  1.5893],\n",
      "        [ 1.9307,  0.0167,  0.5829],\n",
      "        [ 1.6073,  0.3414, -0.1228],\n",
      "        [-0.1569,  1.5402,  1.2369]])\n",
      "tensor([[ 0.3062,  1.0174,  0.9678],\n",
      "        [ 0.9446, -0.9590,  1.5893],\n",
      "        [ 1.9307,  0.0167,  0.5829],\n",
      "        [ 1.6073,  0.3414, -0.1228],\n",
      "        [-0.1569,  1.5402,  1.2369]])\n",
      "tensor([[ 0.3062,  1.0174,  0.9678],\n",
      "        [ 0.9446, -0.9590,  1.5893],\n",
      "        [ 1.9307,  0.0167,  0.5829],\n",
      "        [ 1.6073,  0.3414, -0.1228],\n",
      "        [-0.1569,  1.5402,  1.2369]])\n",
      "tensor([[ 0.3062,  1.0174,  0.9678],\n",
      "        [ 0.9446, -0.9590,  1.5893],\n",
      "        [ 1.9307,  0.0167,  0.5829],\n",
      "        [ 1.6073,  0.3414, -0.1228],\n",
      "        [-0.1569,  1.5402,  1.2369]])\n",
      "tensor([[ 0.3062,  1.0174,  0.9678],\n",
      "        [ 0.9446, -0.9590,  1.5893],\n",
      "        [ 1.9307,  0.0167,  0.5829],\n",
      "        [ 1.6073,  0.3414, -0.1228],\n",
      "        [-0.1569,  1.5402,  1.2369]])\n",
      "tensor([[ 0.3062,  1.0174,  0.9678],\n",
      "        [ 0.9446, -0.9590,  1.5893],\n",
      "        [ 1.9307,  0.0167,  0.5829],\n",
      "        [ 1.6073,  0.3414, -0.1228],\n",
      "        [-0.1569,  1.5402,  1.2369]])\n"
     ]
    }
   ],
   "source": [
    "y = torch.rand(5, 3)\n",
    "print(x+y)\n",
    "print(torch.add(x, y))\n",
    "\n",
    "# 输出复制给新的变量\n",
    "z = torch.empty(5, 3)\n",
    "print(z)\n",
    "torch.add(x, y, out=z)\n",
    "print(z)\n",
    "z = x + y\n",
    "print(z)\n",
    "\n",
    "# inplace operate: add x to y\n",
    "y.add_(x)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "索引："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.3641,  0.3783,  0.4136],\n",
      "        [ 0.0720, -1.4639,  1.2516],\n",
      "        [ 1.1459, -0.7147, -0.0098],\n",
      "        [ 0.7281, -0.3607, -0.6848],\n",
      "        [-0.8404,  1.4801,  0.6986]])\n",
      "tensor([-0.8404,  1.4801,  0.6986])\n",
      "tensor([-0.3641,  0.0720,  1.1459,  0.7281, -0.8404])\n"
     ]
    }
   ],
   "source": [
    "print(x)\n",
    "print(x[4])\n",
    "print(x[:, 0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "改变形状（内存共享）："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-0.3641,  0.3783,  0.4136,  0.0720, -1.4639,  1.2516,  1.1459, -0.7147,\n",
      "        -0.0098,  0.7281, -0.3607, -0.6848, -0.8404,  1.4801,  0.6986])\n",
      "tensor([[-0.3641,  0.3783,  0.4136,  0.0720, -1.4639],\n",
      "        [ 1.2516,  1.1459, -0.7147, -0.0098,  0.7281],\n",
      "        [-0.3607, -0.6848, -0.8404,  1.4801,  0.6986]])\n"
     ]
    }
   ],
   "source": [
    "y = x.view(15)\n",
    "print(y)\n",
    "y = x.view(-1, 5)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.6359,  1.3783,  1.4136,  1.0720, -0.4639],\n",
      "        [ 2.2516,  2.1459,  0.2853,  0.9902,  1.7281],\n",
      "        [ 0.6393,  0.3152,  0.1596,  2.4801,  1.6986]])\n"
     ]
    }
   ],
   "source": [
    "x += 1\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "改变形状并分配给拥有独立内存的变量:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.6359, 2.3783, 2.4136],\n",
      "        [2.0720, 0.5361, 3.2516],\n",
      "        [3.1459, 1.2853, 1.9902],\n",
      "        [2.7281, 1.6393, 1.3152],\n",
      "        [1.1596, 3.4801, 2.6986]])\n",
      "tensor([ 0.6359,  1.3783,  1.4136,  1.0720, -0.4639,  2.2516,  2.1459,  0.2853,\n",
      "         0.9902,  1.7281,  0.6393,  0.3152,  0.1596,  2.4801,  1.6986])\n"
     ]
    }
   ],
   "source": [
    "y = x.clone().view(15)\n",
    "x += 1\n",
    "print(x)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "将一个标量tensor转换为一个number："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-1.1103])\n",
      "-1.1102626323699951\n"
     ]
    }
   ],
   "source": [
    "z = torch.randn(1)\n",
    "print(z)\n",
    "print(z.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "当两个tensor维度不同时，element-wise operate会触发广播机制："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1, 2]])\n",
      "tensor([[1],\n",
      "        [2],\n",
      "        [3]])\n",
      "tensor([[2, 3],\n",
      "        [3, 4],\n",
      "        [4, 5]])\n"
     ]
    }
   ],
   "source": [
    "x = torch.arange(1, 3).view(1, 2)\n",
    "print(x)\n",
    "y = torch.arange(1, 4).view(3, 1)\n",
    "print(y)\n",
    "\n",
    "print(x + y)\n",
    "# x --> [[1,2], [1,2], [1,2]]\n",
    "# y --> [[1,1], [2,2], [3,3]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "关于是否共享内存的探讨（+=和inplace操作是共享内存的，一般的赋值运算则不共享内存）："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "140585563645104 140585563643808 False\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor([1, 2])\n",
    "y = torch.tensor([3, 4])\n",
    "id_before = id(y)\n",
    "y = y + x\n",
    "id_after = id(y)\n",
    "print(id_before, id_after, id_before == id_after)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor([1, 2])\n",
    "y = torch.tensor([3, 4])\n",
    "id_before = id(y)\n",
    "y += x\n",
    "id_after = id(y)\n",
    "print(id_before == id_after)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor([1, 2])\n",
    "y = torch.tensor([3, 4])\n",
    "id_before = id(y)\n",
    "y[:] = y + x\n",
    "id_after = id(y)\n",
    "print(id_before == id_after)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor([1, 2])\n",
    "y = torch.tensor([3, 4])\n",
    "id_before = id(y)\n",
    "y.add_(x)\n",
    "id_after = id(y)\n",
    "print(id_before == id_after)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tensor与numpy array之间的转换（from_numpy()：二者共享内存）："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.],\n",
      "        [1., 1., 1.],\n",
      "        [1., 1., 1.],\n",
      "        [1., 1., 1.]]) [[1. 1. 1.]\n",
      " [1. 1. 1.]\n",
      " [1. 1. 1.]\n",
      " [1. 1. 1.]\n",
      " [1. 1. 1.]] \n",
      "\n",
      "tensor([[2., 2., 2.],\n",
      "        [2., 2., 2.],\n",
      "        [2., 2., 2.],\n",
      "        [2., 2., 2.],\n",
      "        [2., 2., 2.]]) [[2. 2. 2.]\n",
      " [2. 2. 2.]\n",
      " [2. 2. 2.]\n",
      " [2. 2. 2.]\n",
      " [2. 2. 2.]] \n",
      "\n",
      "tensor([[3., 3., 3.],\n",
      "        [3., 3., 3.],\n",
      "        [3., 3., 3.],\n",
      "        [3., 3., 3.],\n",
      "        [3., 3., 3.]]) [[3. 3. 3.]\n",
      " [3. 3. 3.]\n",
      " [3. 3. 3.]\n",
      " [3. 3. 3.]\n",
      " [3. 3. 3.]]\n"
     ]
    }
   ],
   "source": [
    "a = torch.ones(5, 3)\n",
    "b = a.numpy()\n",
    "print(a, b, '\\n')\n",
    "\n",
    "a += 1\n",
    "print(a, b, '\\n')\n",
    "b += 1\n",
    "print(a, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1. 1. 1.]\n",
      " [1. 1. 1.]\n",
      " [1. 1. 1.]\n",
      " [1. 1. 1.]\n",
      " [1. 1. 1.]] \n",
      " tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.],\n",
      "        [1., 1., 1.],\n",
      "        [1., 1., 1.],\n",
      "        [1., 1., 1.]], dtype=torch.float64) \n",
      "\n",
      "[[2. 2. 2.]\n",
      " [2. 2. 2.]\n",
      " [2. 2. 2.]\n",
      " [2. 2. 2.]\n",
      " [2. 2. 2.]] \n",
      " tensor([[2., 2., 2.],\n",
      "        [2., 2., 2.],\n",
      "        [2., 2., 2.],\n",
      "        [2., 2., 2.],\n",
      "        [2., 2., 2.]], dtype=torch.float64) \n",
      "\n",
      "[[3. 3. 3.]\n",
      " [3. 3. 3.]\n",
      " [3. 3. 3.]\n",
      " [3. 3. 3.]\n",
      " [3. 3. 3.]] \n",
      " tensor([[3., 3., 3.],\n",
      "        [3., 3., 3.],\n",
      "        [3., 3., 3.],\n",
      "        [3., 3., 3.],\n",
      "        [3., 3., 3.]], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "a = np.ones((5, 3))\n",
    "b = torch.from_numpy(a)\n",
    "print(a, '\\n', b, '\\n')\n",
    "\n",
    "a += 1\n",
    "print(a, '\\n', b, '\\n')\n",
    "b += 1\n",
    "print(a, '\\n', b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "使用tensor.tensor()不会共享内存："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[3. 3. 3.]\n",
      " [3. 3. 3.]\n",
      " [3. 3. 3.]\n",
      " [3. 3. 3.]\n",
      " [3. 3. 3.]] \n",
      " tensor([[3., 3., 3.],\n",
      "        [3., 3., 3.],\n",
      "        [3., 3., 3.],\n",
      "        [3., 3., 3.],\n",
      "        [3., 3., 3.]], dtype=torch.float64) \n",
      "\n",
      "[[4. 4. 4.]\n",
      " [4. 4. 4.]\n",
      " [4. 4. 4.]\n",
      " [4. 4. 4.]\n",
      " [4. 4. 4.]] \n",
      " tensor([[3., 3., 3.],\n",
      "        [3., 3., 3.],\n",
      "        [3., 3., 3.],\n",
      "        [3., 3., 3.],\n",
      "        [3., 3., 3.]], dtype=torch.float64) \n",
      "\n",
      "[[4. 4. 4.]\n",
      " [4. 4. 4.]\n",
      " [4. 4. 4.]\n",
      " [4. 4. 4.]\n",
      " [4. 4. 4.]] \n",
      " tensor([[4., 4., 4.],\n",
      "        [4., 4., 4.],\n",
      "        [4., 4., 4.],\n",
      "        [4., 4., 4.],\n",
      "        [4., 4., 4.]], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "c = torch.tensor(a)\n",
    "print(a, '\\n', c, '\\n')\n",
    "\n",
    "a += 1\n",
    "print(a, '\\n', c, '\\n')\n",
    "c += 1\n",
    "print(a, '\\n', c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 自动求梯度"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 相关概念\n",
    "``tensor.requires_grad``若被设置为true，则该变量上的所有操作都将被追踪。通过调用``.backward``完成所有梯度计算。计算结果被存放到``.grad``属性中。\n",
    "\n",
    "Tensor和Function共同构建了一个记录整个计算过程的非循环图。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 2.],\n",
      "        [3., 4.]], requires_grad=True)\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor([[1., 2.], [3., 4.]], requires_grad=True)\n",
    "print(x)\n",
    "# 直接通过数值创建x，不是任何变量的参数\n",
    "print(x.grad_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[3., 4.],\n",
      "        [5., 6.]], grad_fn=<AddBackward0>)\n",
      "<AddBackward0 object at 0x7fdca0955950>\n"
     ]
    }
   ],
   "source": [
    "y = x + 2\n",
    "print(y)\n",
    "# y是x的函数，有一个加法的grad_fn\n",
    "print(y.grad_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True False\n"
     ]
    }
   ],
   "source": [
    "# is_leaf用来判断一个变量是否是计算图中的叶子结点（自变量）\n",
    "print(x.is_leaf, y.is_leaf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 27.,  48.],\n",
      "        [ 75., 108.]], grad_fn=<MulBackward0>)\n",
      "tensor(64.5000, grad_fn=<MeanBackward0>)\n"
     ]
    }
   ],
   "source": [
    "z = 3 * y**2\n",
    "o = z.mean()\n",
    "print(z)\n",
    "print(o)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "用inplace的方式改变变量的requires_grad属性："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "None\n",
      "<DivBackward0 object at 0x7fdca09554d0>\n"
     ]
    }
   ],
   "source": [
    "a = torch.randn(2, 2)\n",
    "b = (a * 3) / (a - 1)\n",
    "print(a.requires_grad)\n",
    "print(b.grad_fn)\n",
    "\n",
    "a.requires_grad_(True)\n",
    "b = (a * 3) / (a - 1)\n",
    "print(b.grad_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 梯度计算（采用分母记法）\n",
    "因为$o = \\frac{1}{4} \\sum_{i=1}^4 z_i = \\frac{1}{4} \\sum_{i=1}^4 3(x_i + 2)^2$，\n",
    "所以$\\frac{\\partial o}{\\partial x} \\in \\mathbb{R}^{(2 \\times 2) \\times 1} = \\mathbb{R}^{2 \\times 2}$，\n",
    "梯度与自变量同形。且$\\frac{\\partial o}{\\partial x_i} = \\frac{3}{2} (x_i + 2)$。\n",
    "\n",
    "因为$x_1 = 1$，所以$\\frac{\\partial o}{\\partial x_1} = 4.5$；\n",
    "因为$x_2 = 2$，所以$\\frac{\\partial o}{\\partial x_1} = 6$。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[4.5000, 6.0000],\n",
      "        [7.5000, 9.0000]])\n"
     ]
    }
   ],
   "source": [
    "# o是一个标量，无需指定要求导的变量（只有一个）。如果o是张量，要指定o中需要被求导的那个元素\n",
    "o.backward()\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "梯度在反向传播的过程中是累加的。每运行一次反向传播，梯度都会累加之前的梯度。如果不想要这样，可以在执行反向传播之前将梯度清零。\n",
    "$o_2 = \\sum_{i=1}^4 x_i$，$\\frac{\\partial o_2}{\\partial x_i} = x_i$。当$x_1 = 1$时，$o_2 = \\sum_{i=1}^4 x_i = 1$。\n",
    "这个结果累加上上一轮的[[4.5, 6.0], [7.5, 9.0]]，才是真正的输出。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 5.5000,  7.0000],\n",
      "        [ 8.5000, 10.0000]])\n"
     ]
    }
   ],
   "source": [
    "o2 = x.sum()\n",
    "o2.backward()\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 1.],\n",
      "        [1., 1.]])\n"
     ]
    }
   ],
   "source": [
    "# 把梯度清零\n",
    "o3 = x.sum()\n",
    "x.grad.data.zero_()\n",
    "o3.backward()\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "为了简化反向传播，Torch仅允许**标量对张量**求导，不允许张量对张量求导。也就是说，因变量必须是一个标量。可是很多情况下因变量并不是张量，怎么办？\n",
    "\n",
    "设$y = f(x)$是个张量，$w$是个和$y$同形的张量，则``y.backward(w)``的含义是$\\frac{\\partial l}{\\partial x}$，\n",
    "其中$l = \\sum_{i} w_i y_i$。因此结果仍然是个与$x$同形的张量。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[2., 4.],\n",
      "        [6., 8.]], grad_fn=<ViewBackward>)\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor([1., 2., 3., 4.], requires_grad=True)\n",
    "y = 2 * x\n",
    "z = y.view(2, 2)\n",
    "print(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([2.0000, 0.2000, 0.0200, 0.0020])\n"
     ]
    }
   ],
   "source": [
    "w = torch.tensor([[1., 0.1], [0.01, 0.001]])\n",
    "z.backward(w)\n",
    "\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如果中断梯度的跟踪，那么在后续的梯度计算中，被中断的变量不会参与进来。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "tensor(1., grad_fn=<PowBackward0>) True\n",
      "tensor(1.) False\n",
      "tensor(2., grad_fn=<AddBackward0>) True\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor(1., requires_grad=True)\n",
    "y1 = x ** 2\n",
    "with torch.no_grad():\n",
    "    y2 = x ** 3\n",
    "y3 = y1 + y2\n",
    "print(x.requires_grad)\n",
    "print(y1, y1.requires_grad)\n",
    "print(y2, y2.requires_grad)\n",
    "print(y3, y3.requires_grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "下面的结果之所以是2是因为：$y_3 = y_1 + terminated(y_2) = y_1 = x^2$，\n",
    "所以$\\frac{\\partial y_3}{\\partial x} = 2 x$。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2.)\n"
     ]
    }
   ],
   "source": [
    "y3.backward()\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在不影响后续梯度的计算的前提下，如何修改自变量的数值？直接对``tensor.data``属性进行操作。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.])\n",
      "False\n",
      "tensor([100.], requires_grad=True)\n",
      "tensor([2.])\n"
     ]
    }
   ],
   "source": [
    "x = torch.ones(1,requires_grad=True)\n",
    "\n",
    "print(x.data) # 还是一个tensor\n",
    "print(x.data.requires_grad) # 但是已经是独立于计算图之外\n",
    "\n",
    "y = 2 * x\n",
    "x.data *= 100 # 只改变了值，不会记录在计算图，所以不会影响梯度传播\n",
    "\n",
    "y.backward()\n",
    "print(x) # 更改data的值也会影响tensor的值\n",
    "print(x.grad)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
