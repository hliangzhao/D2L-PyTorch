{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 基本数据操作"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 1.3.1\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(\"PyTorch version:\", torch.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 创建PyTorch Tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.1210e-44, 0.0000e+00, 0.0000e+00],\n",
      "        [0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
      "        [0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
      "        [0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
      "        [0.0000e+00, 0.0000e+00, 0.0000e+00]])\n"
     ]
    }
   ],
   "source": [
    "x = torch.empty(5, 3)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.3300, 0.1686, 0.6637],\n",
      "        [0.4694, 0.6479, 0.8125],\n",
      "        [0.1262, 0.6053, 0.0463],\n",
      "        [0.3959, 0.9953, 0.0712],\n",
      "        [0.0091, 0.5611, 0.3800]])\n"
     ]
    }
   ],
   "source": [
    "x = torch.rand(5, 3)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "指定数据类型："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0, 0, 0],\n",
      "        [0, 0, 0],\n",
      "        [0, 0, 0],\n",
      "        [0, 0, 0],\n",
      "        [0, 0, 0]])\n",
      "torch.int64\n"
     ]
    }
   ],
   "source": [
    "x = torch.zeros(5, 3, dtype=torch.long)\n",
    "print(x)\n",
    "print(x.dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "从现有array创建："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([5.5000, 3.0000])\n",
      "tensor([[5.2000, 2.0000],\n",
      "        [1.2000, 9.0000]])\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor([5.5, 3])\n",
    "print(x)\n",
    "y = torch.tensor([[5.2, 2], [1.2, 9]])\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "从现有的tensor创建："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.],\n",
      "        [1., 1., 1.],\n",
      "        [1., 1., 1.],\n",
      "        [1., 1., 1.]])\n",
      "tensor([[ 1.5636,  0.2672, -1.2523],\n",
      "        [-0.7411, -0.0384,  0.3475],\n",
      "        [-0.0169, -0.3022, -1.4429],\n",
      "        [ 0.5407, -0.2323,  0.7721],\n",
      "        [ 1.7933, -2.3761,  1.7563]])\n"
     ]
    }
   ],
   "source": [
    "x = x.new_ones(5, 3)\n",
    "print(x)\n",
    "x = torch.randn_like(x, dtype=torch.float)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "获取tensor形状："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 3])\n",
      "torch.Size([5, 3])\n"
     ]
    }
   ],
   "source": [
    "print(x.shape)\n",
    "print(x.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 PyTorch Tensor操作"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "加法："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 2.3121,  0.6403, -1.0691],\n",
      "        [-0.4027,  0.0817,  0.5168],\n",
      "        [ 0.9239,  0.0718, -0.5596],\n",
      "        [ 1.4712,  0.0138,  1.3052],\n",
      "        [ 2.2621, -1.4883,  2.0221]])\n",
      "tensor([[ 2.3121,  0.6403, -1.0691],\n",
      "        [-0.4027,  0.0817,  0.5168],\n",
      "        [ 0.9239,  0.0718, -0.5596],\n",
      "        [ 1.4712,  0.0138,  1.3052],\n",
      "        [ 2.2621, -1.4883,  2.0221]])\n",
      "tensor([[ 2.2969,  0.6403, -1.0691],\n",
      "        [-0.4027,  0.0817,  0.5168],\n",
      "        [ 0.9239,  0.0718, -0.5596],\n",
      "        [ 1.4712,  0.0138,  1.3052],\n",
      "        [ 2.2621, -1.4883,  2.0221]])\n",
      "tensor([[ 2.3121,  0.6403, -1.0691],\n",
      "        [-0.4027,  0.0817,  0.5168],\n",
      "        [ 0.9239,  0.0718, -0.5596],\n",
      "        [ 1.4712,  0.0138,  1.3052],\n",
      "        [ 2.2621, -1.4883,  2.0221]])\n",
      "tensor([[ 2.3121,  0.6403, -1.0691],\n",
      "        [-0.4027,  0.0817,  0.5168],\n",
      "        [ 0.9239,  0.0718, -0.5596],\n",
      "        [ 1.4712,  0.0138,  1.3052],\n",
      "        [ 2.2621, -1.4883,  2.0221]])\n",
      "tensor([[ 2.3121,  0.6403, -1.0691],\n",
      "        [-0.4027,  0.0817,  0.5168],\n",
      "        [ 0.9239,  0.0718, -0.5596],\n",
      "        [ 1.4712,  0.0138,  1.3052],\n",
      "        [ 2.2621, -1.4883,  2.0221]])\n"
     ]
    }
   ],
   "source": [
    "y = torch.rand(5, 3)\n",
    "print(x+y)\n",
    "print(torch.add(x, y))\n",
    "\n",
    "# 输出复制给新的变量\n",
    "z = torch.empty(5, 3)\n",
    "print(z)\n",
    "torch.add(x, y, out=z)\n",
    "print(z)\n",
    "z = x + y\n",
    "print(z)\n",
    "\n",
    "# inplace operate: add x to y\n",
    "y.add_(x)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "索引："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1.5636,  0.2672, -1.2523],\n",
      "        [-0.7411, -0.0384,  0.3475],\n",
      "        [-0.0169, -0.3022, -1.4429],\n",
      "        [ 0.5407, -0.2323,  0.7721],\n",
      "        [ 1.7933, -2.3761,  1.7563]])\n",
      "tensor([ 1.7933, -2.3761,  1.7563])\n",
      "tensor([ 1.5636, -0.7411, -0.0169,  0.5407,  1.7933])\n"
     ]
    }
   ],
   "source": [
    "print(x)\n",
    "print(x[4])\n",
    "print(x[:, 0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "改变形状（内存共享）："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 1.5636,  0.2672, -1.2523, -0.7411, -0.0384,  0.3475, -0.0169, -0.3022,\n",
      "        -1.4429,  0.5407, -0.2323,  0.7721,  1.7933, -2.3761,  1.7563])\n",
      "tensor([[ 1.5636,  0.2672, -1.2523, -0.7411, -0.0384],\n",
      "        [ 0.3475, -0.0169, -0.3022, -1.4429,  0.5407],\n",
      "        [-0.2323,  0.7721,  1.7933, -2.3761,  1.7563]])\n"
     ]
    }
   ],
   "source": [
    "y = x.view(15)\n",
    "print(y)\n",
    "y = x.view(-1, 5)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 2.5636,  1.2672, -0.2523,  0.2589,  0.9616],\n",
      "        [ 1.3475,  0.9831,  0.6978, -0.4429,  1.5407],\n",
      "        [ 0.7677,  1.7721,  2.7933, -1.3761,  2.7563]])\n"
     ]
    }
   ],
   "source": [
    "x += 1\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "改变形状并分配给拥有独立内存的变量:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 3.5636,  2.2672,  0.7477],\n",
      "        [ 1.2589,  1.9616,  2.3475],\n",
      "        [ 1.9831,  1.6978,  0.5571],\n",
      "        [ 2.5407,  1.7677,  2.7721],\n",
      "        [ 3.7933, -0.3761,  3.7563]])\n",
      "tensor([ 2.5636,  1.2672, -0.2523,  0.2589,  0.9616,  1.3475,  0.9831,  0.6978,\n",
      "        -0.4429,  1.5407,  0.7677,  1.7721,  2.7933, -1.3761,  2.7563])\n"
     ]
    }
   ],
   "source": [
    "y = x.clone().view(15)\n",
    "x += 1\n",
    "print(x)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "将一个标量tensor转换为一个number："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.2421])\n",
      "0.2421070635318756\n"
     ]
    }
   ],
   "source": [
    "z = torch.randn(1)\n",
    "print(z)\n",
    "print(z.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "当两个tensor维度不同时，element-wise operate会触发广播机制："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1, 2]])\n",
      "tensor([[1],\n",
      "        [2],\n",
      "        [3]])\n",
      "tensor([[2, 3],\n",
      "        [3, 4],\n",
      "        [4, 5]])\n"
     ]
    }
   ],
   "source": [
    "x = torch.arange(1, 3).view(1, 2)\n",
    "print(x)\n",
    "y = torch.arange(1, 4).view(3, 1)\n",
    "print(y)\n",
    "\n",
    "print(x + y)\n",
    "# x --> [[1,2], [1,2], [1,2]]\n",
    "# y --> [[1,1], [2,2], [3,3]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "关于是否共享内存的探讨（+=和inplace操作是共享内存的，一般的赋值运算则不共享内存）："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "140255165048624 140255164751536 False\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor([1, 2])\n",
    "y = torch.tensor([3, 4])\n",
    "id_before = id(y)\n",
    "y = y + x\n",
    "id_after = id(y)\n",
    "print(id_before, id_after, id_before == id_after)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor([1, 2])\n",
    "y = torch.tensor([3, 4])\n",
    "id_before = id(y)\n",
    "y += x\n",
    "id_after = id(y)\n",
    "print(id_before == id_after)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor([1, 2])\n",
    "y = torch.tensor([3, 4])\n",
    "id_before = id(y)\n",
    "y[:] = y + x\n",
    "id_after = id(y)\n",
    "print(id_before == id_after)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor([1, 2])\n",
    "y = torch.tensor([3, 4])\n",
    "id_before = id(y)\n",
    "y.add_(x)\n",
    "id_after = id(y)\n",
    "print(id_before == id_after)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tensor与numpy array之间的转换（from_numpy()：二者共享内存）："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.],\n",
      "        [1., 1., 1.],\n",
      "        [1., 1., 1.],\n",
      "        [1., 1., 1.]]) [[1. 1. 1.]\n",
      " [1. 1. 1.]\n",
      " [1. 1. 1.]\n",
      " [1. 1. 1.]\n",
      " [1. 1. 1.]] \n",
      "\n",
      "tensor([[2., 2., 2.],\n",
      "        [2., 2., 2.],\n",
      "        [2., 2., 2.],\n",
      "        [2., 2., 2.],\n",
      "        [2., 2., 2.]]) [[2. 2. 2.]\n",
      " [2. 2. 2.]\n",
      " [2. 2. 2.]\n",
      " [2. 2. 2.]\n",
      " [2. 2. 2.]] \n",
      "\n",
      "tensor([[3., 3., 3.],\n",
      "        [3., 3., 3.],\n",
      "        [3., 3., 3.],\n",
      "        [3., 3., 3.],\n",
      "        [3., 3., 3.]]) [[3. 3. 3.]\n",
      " [3. 3. 3.]\n",
      " [3. 3. 3.]\n",
      " [3. 3. 3.]\n",
      " [3. 3. 3.]]\n"
     ]
    }
   ],
   "source": [
    "a = torch.ones(5, 3)\n",
    "b = a.numpy()\n",
    "print(a, b, '\\n')\n",
    "\n",
    "a += 1\n",
    "print(a, b, '\\n')\n",
    "b += 1\n",
    "print(a, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1. 1. 1.]\n",
      " [1. 1. 1.]\n",
      " [1. 1. 1.]\n",
      " [1. 1. 1.]\n",
      " [1. 1. 1.]] \n",
      " tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.],\n",
      "        [1., 1., 1.],\n",
      "        [1., 1., 1.],\n",
      "        [1., 1., 1.]], dtype=torch.float64) \n",
      "\n",
      "[[2. 2. 2.]\n",
      " [2. 2. 2.]\n",
      " [2. 2. 2.]\n",
      " [2. 2. 2.]\n",
      " [2. 2. 2.]] \n",
      " tensor([[2., 2., 2.],\n",
      "        [2., 2., 2.],\n",
      "        [2., 2., 2.],\n",
      "        [2., 2., 2.],\n",
      "        [2., 2., 2.]], dtype=torch.float64) \n",
      "\n",
      "[[3. 3. 3.]\n",
      " [3. 3. 3.]\n",
      " [3. 3. 3.]\n",
      " [3. 3. 3.]\n",
      " [3. 3. 3.]] \n",
      " tensor([[3., 3., 3.],\n",
      "        [3., 3., 3.],\n",
      "        [3., 3., 3.],\n",
      "        [3., 3., 3.],\n",
      "        [3., 3., 3.]], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "a = np.ones((5, 3))\n",
    "b = torch.from_numpy(a)\n",
    "print(a, '\\n', b, '\\n')\n",
    "\n",
    "a += 1\n",
    "print(a, '\\n', b, '\\n')\n",
    "b += 1\n",
    "print(a, '\\n', b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "使用tensor.tensor()不会共享内存："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[3. 3. 3.]\n",
      " [3. 3. 3.]\n",
      " [3. 3. 3.]\n",
      " [3. 3. 3.]\n",
      " [3. 3. 3.]] \n",
      " tensor([[3., 3., 3.],\n",
      "        [3., 3., 3.],\n",
      "        [3., 3., 3.],\n",
      "        [3., 3., 3.],\n",
      "        [3., 3., 3.]], dtype=torch.float64) \n",
      "\n",
      "[[4. 4. 4.]\n",
      " [4. 4. 4.]\n",
      " [4. 4. 4.]\n",
      " [4. 4. 4.]\n",
      " [4. 4. 4.]] \n",
      " tensor([[3., 3., 3.],\n",
      "        [3., 3., 3.],\n",
      "        [3., 3., 3.],\n",
      "        [3., 3., 3.],\n",
      "        [3., 3., 3.]], dtype=torch.float64) \n",
      "\n",
      "[[4. 4. 4.]\n",
      " [4. 4. 4.]\n",
      " [4. 4. 4.]\n",
      " [4. 4. 4.]\n",
      " [4. 4. 4.]] \n",
      " tensor([[4., 4., 4.],\n",
      "        [4., 4., 4.],\n",
      "        [4., 4., 4.],\n",
      "        [4., 4., 4.],\n",
      "        [4., 4., 4.]], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "c = torch.tensor(a)\n",
    "print(a, '\\n', c, '\\n')\n",
    "\n",
    "a += 1\n",
    "print(a, '\\n', c, '\\n')\n",
    "c += 1\n",
    "print(a, '\\n', c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 自动求梯度"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 相关概念\n",
    "``tensor.requires_grad``若被设置为true，则该变量上的所有操作都将被追踪。通过调用``.backward``完成所有梯度计算。计算结果被存放到``.grad``属性中。\n",
    "\n",
    "Tensor和Function共同构建了一个记录整个计算过程的非循环图。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 2.],\n",
      "        [3., 4.]], requires_grad=True)\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor([[1., 2.], [3., 4.]], requires_grad=True)\n",
    "print(x)\n",
    "# 直接通过数值创建x，不是任何变量的参数\n",
    "print(x.grad_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[3., 4.],\n",
      "        [5., 6.]], grad_fn=<AddBackward0>)\n",
      "<AddBackward0 object at 0x7f8fb3ff1b10>\n"
     ]
    }
   ],
   "source": [
    "y = x + 2\n",
    "print(y)\n",
    "# y是x的函数，有一个加法的grad_fn\n",
    "print(y.grad_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True False\n"
     ]
    }
   ],
   "source": [
    "# is_leaf用来判断一个变量是否是计算图中的叶子结点（自变量）\n",
    "print(x.is_leaf, y.is_leaf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 27.,  48.],\n",
      "        [ 75., 108.]], grad_fn=<MulBackward0>)\n",
      "tensor(64.5000, grad_fn=<MeanBackward0>)\n"
     ]
    }
   ],
   "source": [
    "z = 3 * y**2\n",
    "o = z.mean()\n",
    "print(z)\n",
    "print(o)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "用inplace的方式改变变量的requires_grad属性："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "None\n",
      "<DivBackward0 object at 0x7f8fb3835d10>\n"
     ]
    }
   ],
   "source": [
    "a = torch.randn(2, 2)\n",
    "b = (a * 3) / (a - 1)\n",
    "print(a.requires_grad)\n",
    "print(b.grad_fn)\n",
    "\n",
    "a.requires_grad_(True)\n",
    "b = (a * 3) / (a - 1)\n",
    "print(b.grad_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 梯度计算（采用分母记法）\n",
    "因为$o = \\frac{1}{4} \\sum_{i=1}^4 z_i = \\frac{1}{4} \\sum_{i=1}^4 3(x_i + 2)^2$，\n",
    "所以$\\frac{\\partial o}{\\partial x} \\in \\mathbb{R}^{(2 \\times 2) \\times 1} = \\mathbb{R}^{2 \\times 2}$，\n",
    "梯度与自变量同形。且$\\frac{\\partial o}{\\partial x_i} = \\frac{3}{2} (x_i + 2)$。\n",
    "\n",
    "因为$x_1 = 1$，所以$\\frac{\\partial o}{\\partial x_1} = 4.5$；\n",
    "因为$x_2 = 2$，所以$\\frac{\\partial o}{\\partial x_1} = 6$。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[4.5000, 6.0000],\n",
      "        [7.5000, 9.0000]])\n"
     ]
    }
   ],
   "source": [
    "# o是一个标量，无需指定要求导的变量（只有一个）。如果o是张量，要指定o中需要被求导的那个元素\n",
    "o.backward()\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "梯度在反向传播的过程中是累加的。每运行一次反向传播，梯度都会累加之前的梯度。如果不想要这样，可以在执行反向传播之前将梯度清零。\n",
    "$o_2 = \\sum_{i=1}^4 x_i$，$\\frac{\\partial o_2}{\\partial x_i} = x_i$。当$x_1 = 1$时，$o_2 = \\sum_{i=1}^4 x_i = 1$。\n",
    "这个结果累加上上一轮的[[4.5, 6.0], [7.5, 9.0]]，才是真正的输出。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 5.5000,  7.0000],\n",
      "        [ 8.5000, 10.0000]])\n"
     ]
    }
   ],
   "source": [
    "o2 = x.sum()\n",
    "o2.backward()\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 1.],\n",
      "        [1., 1.]])\n"
     ]
    }
   ],
   "source": [
    "# 把梯度清零\n",
    "o3 = x.sum()\n",
    "x.grad.data.zero_()\n",
    "o3.backward()\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "为了简化反向传播，Torch仅允许**标量对张量**求导，不允许张量对张量求导。也就是说，因变量必须是一个标量。可是很多情况下因变量并不是张量，怎么办？\n",
    "\n",
    "设$y = f(x)$是个张量，$w$是个和$y$同形的张量，则``y.backward(w)``的含义是$\\frac{\\partial l}{\\partial x}$，\n",
    "其中$l = \\sum_{i} w_i y_i$。因此结果仍然是个与$x$同形的张量。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[2., 4.],\n",
      "        [6., 8.]], grad_fn=<ViewBackward>)\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor([1., 2., 3., 4.], requires_grad=True)\n",
    "y = 2 * x\n",
    "z = y.view(2, 2)\n",
    "print(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([2.0000, 0.2000, 0.0200, 0.0020])\n"
     ]
    }
   ],
   "source": [
    "w = torch.tensor([[1., 0.1], [0.01, 0.001]])\n",
    "z.backward(w)\n",
    "\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如果中断梯度的跟踪，那么在后续的梯度计算中，被中断的变量不会参与进来。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "tensor(1., grad_fn=<PowBackward0>) True\n",
      "tensor(1.) False\n",
      "tensor(2., grad_fn=<AddBackward0>) True\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor(1., requires_grad=True)\n",
    "y1 = x ** 2\n",
    "with torch.no_grad():\n",
    "    y2 = x ** 3\n",
    "y3 = y1 + y2\n",
    "print(x.requires_grad)\n",
    "print(y1, y1.requires_grad)\n",
    "print(y2, y2.requires_grad)\n",
    "print(y3, y3.requires_grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "下面的结果之所以是2是因为：$y_3 = y_1 + terminated(y_2) = y_1 = x^2$，\n",
    "所以$\\frac{\\partial y_3}{\\partial x} = 2 x$。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2.)\n"
     ]
    }
   ],
   "source": [
    "y3.backward()\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在不影响后续梯度的计算的前提下，如何修改自变量的数值？直接对``tensor.data``属性进行操作。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.])\n",
      "False\n",
      "tensor([100.], requires_grad=True)\n",
      "tensor([2.])\n"
     ]
    }
   ],
   "source": [
    "x = torch.ones(1,requires_grad=True)\n",
    "\n",
    "print(x.data) # 还是一个tensor\n",
    "print(x.data.requires_grad) # 但是已经是独立于计算图之外\n",
    "\n",
    "y = 2 * x\n",
    "x.data *= 100 # 只改变了值，不会记录在计算图，所以不会影响梯度传播\n",
    "\n",
    "y.backward()\n",
    "print(x) # 更改data的值也会影响tensor的值\n",
    "print(x.grad)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
