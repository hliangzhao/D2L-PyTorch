{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 模型参数的访问、初始化与共享"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "定义含单个隐藏层的感知机："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (0): Linear(in_features=4, out_features=3, bias=True)\n",
      "  (1): ReLU()\n",
      "  (2): Linear(in_features=3, out_features=1, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.init as init\n",
    "\n",
    "# 使用Sequential构造模型会默认执行初始化\n",
    "net = nn.Sequential(nn.Linear(4, 3), nn.ReLU(), nn.Linear(3, 1))\n",
    "print(net)\n",
    "\n",
    "X = torch.rand(2, 4)\n",
    "Y = net(X).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 通过net.parameters()或nn.named_parameters()访问参数\n",
    "参数的类型为torch.nn.parameter.Parameter，这是tensor的子类，该类型的实例会被自动加入到模型的参数列表中。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'generator'>\n",
      "0.weight torch.Size([3, 4]) <class 'torch.nn.parameter.Parameter'> \n",
      " Parameter containing:\n",
      "tensor([[ 0.0346,  0.2906,  0.1959, -0.1518],\n",
      "        [-0.0540,  0.4834,  0.2244,  0.2504],\n",
      "        [ 0.1455, -0.1077, -0.0040, -0.0552]], requires_grad=True) \n",
      "\n",
      "0.bias torch.Size([3]) <class 'torch.nn.parameter.Parameter'> \n",
      " Parameter containing:\n",
      "tensor([ 0.0441, -0.4389,  0.1332], requires_grad=True) \n",
      "\n",
      "2.weight torch.Size([1, 3]) <class 'torch.nn.parameter.Parameter'> \n",
      " Parameter containing:\n",
      "tensor([[0.0758, 0.3708, 0.3032]], requires_grad=True) \n",
      "\n",
      "2.bias torch.Size([1]) <class 'torch.nn.parameter.Parameter'> \n",
      " Parameter containing:\n",
      "tensor([0.5126], requires_grad=True) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(type(net.named_parameters()))  # type is generator\n",
    "for name, param in net.named_parameters():\n",
    "    print(name, param.size(), type(param), '\\n', param, '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w1 \n",
      " Parameter containing:\n",
      "tensor([[0.8450, 0.5001, 0.7558, 0.7901],\n",
      "        [0.2242, 0.0298, 0.2684, 0.5284],\n",
      "        [0.2449, 0.8873, 0.8072, 0.2524],\n",
      "        [0.3379, 0.0736, 0.4378, 0.8302]], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "class MyNet(nn.Module):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(MyNet, self).__init__(**kwargs)\n",
    "        self.w1 = torch.nn.parameter.Parameter(torch.rand(4, 4))\n",
    "        self.w2 = torch.rand(4, 4)\n",
    "\n",
    "n = MyNet()\n",
    "for name, param in n.named_parameters():\n",
    "    # 输出不包含w2\n",
    "    print(name, '\\n', param)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "通过.data和.grad访问数值和梯度值："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.0346,  0.2906,  0.1959, -0.1518],\n",
      "        [-0.0540,  0.4834,  0.2244,  0.2504],\n",
      "        [ 0.1455, -0.1077, -0.0040, -0.0552]])\n",
      "None\n",
      "tensor([[0.0559, 0.0652, 0.0695, 0.0154],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.2237, 0.2610, 0.2780, 0.0618]])\n"
     ]
    }
   ],
   "source": [
    "weight_0 = list(net[0].parameters())[0]\n",
    "print(weight_0.data)\n",
    "print(weight_0.grad) # 反向传播前梯度为None\n",
    "Y.backward()\n",
    "print(weight_0.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 模型参数的初始化\n",
    "通过torch.nn.init()来实现。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.weight tensor([[-0.0043,  0.0049,  0.0025, -0.0073],\n",
      "        [-0.0070,  0.0055, -0.0011,  0.0107],\n",
      "        [ 0.0039, -0.0043, -0.0109, -0.0051]])\n",
      "0.bias tensor([0.6006, 0.2985, 0.9304])\n",
      "2.weight tensor([[ 0.0048, -0.0116,  0.0120]])\n",
      "2.bias tensor([0.9254])\n"
     ]
    }
   ],
   "source": [
    "for name, param in net.named_parameters():\n",
    "    if 'weight' in name:\n",
    "        init.normal_(param, mean=0, std=0.01)\n",
    "        print(name, param.data)\n",
    "    if 'bias' in name:\n",
    "        init.uniform_(param, a=-1., b=1.)\n",
    "        print(name, param.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "实现一个自定义的初始化方法。在下面的例子里，我们令权重有一半概率初始化为0，有另一半概率初始化为[-10,-5][−10,−5]和[5,10][5,10]两个区间里均匀分布的随机数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.weight tensor([[-0.0000, -7.9060,  0.0000,  8.1568],\n",
      "        [-6.9980,  0.0000,  5.2687, -0.0000],\n",
      "        [ 0.0000,  0.0000, -0.0000, -5.3343]])\n",
      "2.weight tensor([[-0., -0., -0.]])\n"
     ]
    }
   ],
   "source": [
    "def init_weight_(tensor):\n",
    "    with torch.no_grad():\n",
    "        tensor.uniform_(-10, 10)\n",
    "        # 每个元素乘0或者乘1\n",
    "        tensor *= (tensor.abs() >= 5).float()\n",
    "\n",
    "for name, param in net.named_parameters():\n",
    "    if 'weight' in name:\n",
    "        init_weight_(param)\n",
    "        print(name, param.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 模型参数的共享"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "传入Sequential的模块是同一个Module的实例："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (0): Linear(in_features=1, out_features=1, bias=False)\n",
      "  (1): Linear(in_features=1, out_features=1, bias=False)\n",
      ")\n",
      "0.weight tensor([[3.]])\n"
     ]
    }
   ],
   "source": [
    "linear = nn.Linear(1, 1, bias=False)\n",
    "net = nn.Sequential(linear, linear)\n",
    "print(net)\n",
    "for name, param in net.named_parameters():\n",
    "    if 'weight' in name:\n",
    "        init.constant_(param, val=3)\n",
    "        print(name, param.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "# 指向的是同一个实例，因此对应的参数是两个module共享的\n",
    "print(id(net[0]) == id(net[1]))\n",
    "print(id(net[0].weight) == id(net[1].weight))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "因为模型参数里包含了梯度，所以在反向传播计算时，这些共享的参数的梯度是累加的："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y = 9.000000\n",
      "tensor([[6.]])\n"
     ]
    }
   ],
   "source": [
    "x = torch.ones(1, 1)\n",
    "y = net(x).sum()    # y = 3 * (3 * x)\n",
    "print('y = %f' % y)\n",
    "y.backward()\n",
    "# grad <- grad + 3重复了两次（每个layer重复了一次，这个结果被累加）\n",
    "print(net[0].weight.grad)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
