{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 编码器—解码器（seq2seq）\n",
    "在自然语言处理的很多应用中，输入和输出都可以是不定长序列。以机器翻译为例，输入可以是一段不定长的英语文本序列，输出也是一段不定长的法语文本序列，例如\n",
    "\n",
    "> 英语输入：“They”、“are”、“watching”、“.”\n",
    "\n",
    "> 法语输出：“Ils”、“regardent”、“.”\n",
    "\n",
    "当输入和输出都是不定长序列时，我们可以使用编码器—解码器（encoder-decoder）或者seq2seq模型。这两个模型本质上都用到了两个循环神经网络，分别叫做编码器和解码器。编码器用来分析输入序列，解码器用来生成输出序列。\n",
    "\n",
    "下图描述了使用编码器—解码器将上述英语句子翻译成法语句子的一种方法。在训练数据集中，我们可以在每个句子后附上特殊符号“&lt;eos&gt;”（end of sequence）以表示序列的终止。编码器每个时间步的输入依次为英语句子中的单词、标点和特殊符号“&lt;eos&gt;”。图中使用了编码器在**最终时间步的隐藏状态**作为输入句子的编码信息。解码器在各个时间步中使用输入句子的编码信息、上个时间步的输出以及隐藏状态作为输入。我们希望解码器在各个时间步能正确依次输出翻译后的法语单词、标点和特殊符号\"<eos>\"。此外，解码器在**最初时间步**的输入用到了一个表示序列开始的特殊符号\"<bos>\"（beginning of sequence）。\n",
    "\n",
    "![avatar](../resource/seq2seq.svg)\n",
    "\n",
    "\n",
    "接下来分别介绍编码器和解码器的定义。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1 编码器\n",
    "\n",
    "编码器的作用是把一个不定长的输入序列的信息编码为一个定长的背景变量$\\boldsymbol{c}$，常用的编码器是循环神经网络。\n",
    "\n",
    "以批量大小为1的时序数据样本为例，假设输入序列是$x_1,\\ldots,x_T$，其中$x_i$是输入句子中的第$i$个词。在时间步$t$，循环神经网络将$x_t$的特征向量$\\boldsymbol{x}_t$和上个时间步的隐藏状态$\\boldsymbol{h}_{t-1}$作为输入，并得到当前时间步的隐藏状态$\\boldsymbol{h}_t$。我们可以用函数$f$表达循环神经网络隐藏层的变换：\n",
    "\n",
    "$$\n",
    "\\boldsymbol{h}_t = f(\\boldsymbol{x}_t, \\boldsymbol{h}_{t-1}).\n",
    "$$\n",
    "\n",
    "接下来，编码器通过自定义函数$q$将各个时间步的隐藏状态变换为背景变量\n",
    "\n",
    "$$\n",
    "\\boldsymbol{c} =  q(\\boldsymbol{h}_1, \\ldots, \\boldsymbol{h}_T).\n",
    "$$\n",
    "\n",
    "例如，**当选择$q(\\boldsymbol{h}_1, \\ldots, \\boldsymbol{h}_T) = \\boldsymbol{h}_T$时，背景变量是输入序列最终时间步的隐藏状态$\\boldsymbol{h}_T$**。\n",
    "\n",
    "以上描述的编码器是一个单向的循环神经网络，每个时间步的隐藏状态只取决于该时间步及之前的输入子序列。也可以使用双向循环神经网络构造编码器。在这种情况下，编码器每个时间步的隐藏状态同时取决于该时间步之前和之后的子序列（包括当前时间步的输入），并编码了整个序列的信息。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2 解码器\n",
    "编码器输出的背景变量$\\boldsymbol{c}$编码了整个输入序列$x_1, \\ldots, x_T$的信息。给定训练样本中的输出序列$y_1, y_2, \\ldots, y_{T'}$，对每个时间步$t'$（符号与输入序列或编码器的时间步$t$有区别），解码器输出$y_{t'}$的条件概率将基于之前的输出序列$y_1,\\ldots,y_{t'-1}$和背景变量$\\boldsymbol{c}$，即$P(y_{t'} \\mid y_1, \\ldots, y_{t'-1}, \\boldsymbol{c})$。\n",
    "\n",
    "为此，我们可以使用另一个循环神经网络作为解码器。在输出序列的时间步$t^\\prime$，解码器将上一时间步的输出$y_{t^\\prime-1}$以及背景变量$\\boldsymbol{c}$作为输入，并将它们与上一时间步的隐藏状态$\\boldsymbol{s}_{t^\\prime-1}$变换为当前时间步的隐藏状态$\\boldsymbol{s}_{t^\\prime}$。因此，我们可以用函数$g$表达解码器隐藏层的变换：\n",
    "\n",
    "$$\n",
    "\\boldsymbol{s}_{t^\\prime} = g(y_{t^\\prime-1}, \\boldsymbol{c}, \\boldsymbol{s}_{t^\\prime-1}).\n",
    "$$\n",
    "\n",
    "有了解码器的隐藏状态后，我们可以使用自定义的输出层和softmax运算来计算$P(y_{t^\\prime} \\mid y_1, \\ldots, y_{t^\\prime-1}, \\boldsymbol{c})$。例如，基于当前时间步的解码器隐藏状态 $\\boldsymbol{s}_{t^\\prime}$、上一时间步的输出$y_{t^\\prime-1}$以及背景变量$\\boldsymbol{c}$来计算当前时间步输出$y_{t^\\prime}$的概率分布。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3 树搜索\n",
    "接下来说明如何在得到$P(y_{t^\\prime} \\mid y_1, \\ldots, y_{t^\\prime-1}, \\boldsymbol{c})$之后输出不定长的序列。\n",
    "\n",
    "设输出文本词典$\\mathcal{Y}$（包含特殊符号\"&lt;eos&gt;\"）的大小为$\\left|\\mathcal{Y}\\right|$，输出序列的最大长度为$T'$。所有可能的输出序列一共有$\\mathcal{O}(\\left|\\mathcal{Y}\\right|^{T'})$种。这些输出序列中所有特殊符号\"<eos>\"后面的子序列将被舍弃。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1 贪婪搜索\n",
    "一种得到输出序列的方法是贪婪搜索（greedy search）。对于输出序列任一时间步$t'$，我们从$|\\mathcal{Y}|$个词中搜索出条件概率最大的词\n",
    "\n",
    "$$\n",
    "y _ { t ^ { \\prime } } = \\underset { y \\in \\mathcal { Y } } { \\operatorname { argmax } } P \\left( y | y _ { 1 } , \\ldots , y _ { t ^ { \\prime } - 1 } , c \\right)\n",
    "$$\n",
    "\n",
    "作为输出。一旦搜索出\"&lt;eos&gt;\"符号，或者输出序列长度已经达到了最大长度$T'$，便完成输出。\n",
    "\n",
    "我们在描述解码器时提到，基于输入序列生成输出序列的条件概率是$\\prod_{t'=1}^{T'} P(y_{t'} \\mid y_1, \\ldots, y_{t'-1}, \\boldsymbol{c})$。我们将该条件概率最大的输出序列称为最优输出序列。而贪婪搜索的主要问题是不能保证得到最优输出序列。接下来用一个例子说明这一问题。\n",
    "\n",
    "假设输出词典里面有“A”“B”“C”和“&lt;eos&gt;”这4个词。下图中每个时间步下的4个数字分别代表了该时间步生成“A”“B”“C”和“&lt;eos&gt;”这4个词的条件概率。在每个时间步，贪婪搜索选取条件概率最大的词。因此，下图中将生成输出序列“A”“B”“C”“&lt;eos&gt;”。该输出序列的条件概率是$0.5\\times0.4\\times0.4\\times0.6 = 0.048$。\n",
    "\n",
    "![avatar](../resource/s2s_prob1.svg)\n",
    "\n",
    "\n",
    "接下来，观察下图演示的例子。与上图中不同，下图在时间步2中选取了条件概率第二大的词“C”。由于时间步3所基于的时间步1和2的输出子序列由图上图中的“A”“B”变为了下图中的“A”“C”，导致下图中时间步3生成各个词的条件概率发生了变化。在时间步3和4分别选取“B”和“&lt;eos&gt;”，此时的输出序列“A”“C”“B”“&lt;eos&gt;”的条件概率是$0.5\\times0.3\\times0.6\\times0.6=0.054$，大于贪婪搜索得到的输出序列的条件概率。因此，贪婪搜索得到的输出序列“A”“B”“C”“&lt;eos&gt;”并非最优输出序列。\n",
    "\n",
    "![avatar](../resource/s2s_prob2.svg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "贪婪搜索的计算开销是$\\mathcal{O}(\\left|\\mathcal{Y}\\right|T')$。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2 穷举搜索\n",
    "如果目标是得到最优输出序列，可以考虑穷举搜索（exhaustive search）：穷举所有可能的输出序列，输出条件概率最大的序列。\n",
    "此时的计算开销为$\\mathcal{O}(\\left|\\mathcal{Y}\\right|^{T'})$。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.3 树搜索\n",
    "束搜索（beam search）是对贪婪搜索的一个改进算法。它有一个束宽（beam size）超参数。我们将它设为$k$。在时间步1时，选取当前时间步条件概率最大的$k$个词，分别组成$k$个候选输出序列的首词。在之后的每个时间步，基于上个时间步的$k$个候选输出序列，从$k\\left|\\mathcal{Y}\\right|$个可能的输出序列中选取条件概率最大的$k$个，作为该时间步的候选输出序列。最终，我们从各个时间步的候选输出序列中筛选出包含特殊符号“&lt;eos&gt;”的序列，并将它们中所有特殊符号“&lt;eos&gt;”后面的子序列舍弃，得到最终候选输出序列的集合。\n",
    "\n",
    "![avatar](../resource/beam_search.svg)\n",
    "\n",
    "上图演示了束搜索的过程。假设输出序列的词典中只包含5个元素，即$\\mathcal{Y} = \\{A, B, C, D, E\\}$，且其中一个为特殊符号“&lt;eos&gt;”。设束搜索的束宽等于2，输出序列最大长度为3。在输出序列的时间步1时，假设条件概率$P(y_1 \\mid \\boldsymbol{c})$最大的2个词为$A$和$C$。我们在时间步2时将对所有的$y_2 \\in \\mathcal{Y}$都分别计算$P(y_2 \\mid A, \\boldsymbol{c})$和$P(y_2 \\mid C, \\boldsymbol{c})$，并从计算出的10个条件概率中取最大的2个，假设为$P(B \\mid A, \\boldsymbol{c})$和$P(E \\mid C, \\boldsymbol{c})$。那么，我们在时间步3时将对所有的$y_3 \\in \\mathcal{Y}$都分别计算$P(y_3 \\mid A, B, \\boldsymbol{c})$和$P(y_3 \\mid C, E, \\boldsymbol{c})$，并从计算出的10个条件概率中取最大的2个，假设为$P(D \\mid A, B, \\boldsymbol{c})$和$P(D \\mid C, E, \\boldsymbol{c})$。如此一来，我们得到6个候选输出序列：（1）$A$；（2）$C$；（3）$A$、$B$；（4）$C$、$E$；（5）$A$、$B$、$D$和（6）$C$、$E$、$D$。接下来，我们将根据这6个序列得出最终候选输出序列的集合（注意每一个最终候选输出的所有特殊符号“&lt;eos&gt;”后面的子序列需要被舍弃）。\n",
    "\n",
    "在最终候选输出序列的集合中，我们取以下分数最高的序列作为输出序列：\n",
    "\n",
    "$$ \\frac{1}{L^\\alpha} \\log P(y_1, \\ldots, y_{L}) = \\frac{1}{L^\\alpha} \\sum_{t'=1}^L \\log P(y_{t'} \\mid y_1, \\ldots, y_{t'-1}, \\boldsymbol{c}),$$\n",
    "\n",
    "其中$L$为最终候选序列长度，$\\alpha$一般可选为0.75。分母上的$L^\\alpha$是为了惩罚较长序列在以上分数中较多的对数相加项。\n",
    "\n",
    "束搜索的计算开销为$\\mathcal{O}(k\\left|\\mathcal{Y}\\right|T')$，介于贪婪搜索和穷举搜索的计算开销之间。**贪婪搜索可看作是束宽为1的束搜索**。束搜索通过灵活的束宽$k$来权衡计算开销和搜索质量。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4 注意力机制\n",
    "请思考这个翻译的例子：输入为英语序列“They”“are”“watching”“.”，输出为法语序列“Ils”“regardent”“.”。不难想到，解码器在生成输出序列中的每一个词时可能只需利用输入序列某一部分的信息。例如，在输出序列的时间步1，解码器可以主要依赖“They”“are”的信息来生成“Ils”，在时间步2则主要使用来自“watching”的编码信息生成“regardent”，最后在时间步3则直接映射句号“.”。这看上去就像是在解码器的每一时间步**对输入序列中不同时间步的表征或编码信息分配不同的注意力**一样，这就是注意力机制。\n",
    "\n",
    "仍然以循环神经网络为例，注意力机制通过对编码器所有时间步的隐藏状态做加权平均来得到背景变量。解码器在每一时间步调整这些权重，即注意力权重，从而能够在不同时间步分别关注输入序列中的不同部分并编码进相应时间步的背景变量。接下来给出注意力机制的工作原理。\n",
    "\n",
    "回顾seq2seq模型，解码器在时间步$t'$的隐藏状态$\\boldsymbol{s}_{t'} = g(\\boldsymbol{y}_{t'-1}, \\boldsymbol{c}, \\boldsymbol{s}_{t'-1})$，其中$\\boldsymbol{y}_{t'-1}$是上一时间步$t'-1$的输出$y_{t'-1}$的表征，且任一时间步$t'$使用相同的背景变量$\\boldsymbol{c}$。但在注意力机制中，解码器的每一时间步将使用可变的背景变量。记$\\boldsymbol{c}_{t'}$是解码器在时间步$t'$的背景变量，那么解码器在该时间步的隐藏状态可以改写为\n",
    "\n",
    "$$\\boldsymbol{s}_{t'} = g(\\boldsymbol{y}_{t'-1}, \\boldsymbol{c}_{t'}, \\boldsymbol{s}_{t'-1}).$$\n",
    "\n",
    "这里的关键是如何计算背景变量$\\boldsymbol{c}_{t'}$和如何利用它来更新隐藏状态$\\boldsymbol{s}_{t'}$。下面将分别描述这两个关键点。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.1 计算背景变量\n",
    "令编码器在时间步$t$的隐藏状态为$\\boldsymbol{h}_t$，且总时间步数为$T$。那么解码器在时间步$t'$的背景变量为所有编码器隐藏状态的加权平均：\n",
    "\n",
    "$$\n",
    "\\boldsymbol{c}_{t'} = \\sum_{t=1}^T \\alpha_{t' t} \\boldsymbol{h}_t,\n",
    "$$\n",
    "\n",
    "其中给定$t'$时，权重$\\alpha_{t' t}$在$t=1,\\ldots,T$的值是一个概率分布。为了得到概率分布，我们可以使用softmax运算:\n",
    "\n",
    "$$\n",
    "\\alpha_{t' t} = \\frac{\\exp(e_{t' t})}{ \\sum_{k=1}^T \\exp(e_{t' k}) },\\quad t=1,\\ldots,T.\n",
    "$$\n",
    "\n",
    "现在，我们需要定义如何计算上式中softmax运算的输入$e_{t' t}$。由于$e_{t' t}$同时取决于解码器的时间步$t'$和编码器的时间步$t$，我们不妨以解码器在时间步$t'-1$的隐藏状态$\\boldsymbol{s}_{t' - 1}$与编码器在时间步$t$的隐藏状态$\\boldsymbol{h}_t$为输入，并通过函数$a$计算$e_{t' t}$：\n",
    "\n",
    "$$\n",
    "e_{t' t} = a(\\boldsymbol{s}_{t' - 1}, \\boldsymbol{h}_t).\n",
    "$$\n",
    "\n",
    "\n",
    "这里函数$a$有多种选择，如果两个输入向量长度相同，一个简单的选择是计算它们的内积$a(\\boldsymbol{s}, \\boldsymbol{h})=\\boldsymbol{s}^\\top \\boldsymbol{h}$。而最早提出注意力机制的论文则将输入连结后通过含单隐藏层的多层感知机变换：\n",
    "\n",
    "$$\n",
    "a(\\boldsymbol{s}, \\boldsymbol{h}) = \\boldsymbol{v}^\\top \\tanh(\\boldsymbol{W}_s \\boldsymbol{s} + \\boldsymbol{W}_h \\boldsymbol{h}),\n",
    "$$\n",
    "\n",
    "其中$\\boldsymbol{v}$、$\\boldsymbol{W}_s$、$\\boldsymbol{W}_h$都是可以学习的模型参数。\n",
    "\n",
    "下图描绘了注意力机制如何为解码器在时间步2计算背景变量。首先，函数$a$根据解码器在时间步1的隐藏状态和编码器在各个时间步的隐藏状态计算softmax运算的输入。softmax运算输出概率分布并对编码器各个时间步的隐藏状态做加权平均，从而得到背景变量。\n",
    "\n",
    "![avatar](../resource/attention.svg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**矢量化运算**：\n",
    "\n",
    "我们还可以对注意力机制采用更高效的矢量化计算。广义上，注意力机制的输入包括查询项以及一一对应的键项和值项，其中值项是需要加权平均的一组项。在加权平均中，值项的权重来自查询项以及与该值项对应的键项的计算。\n",
    "\n",
    "在上面的例子中，查询项为解码器的隐藏状态，键项和值项均为编码器的隐藏状态。\n",
    "让我们考虑一个常见的简单情形，即编码器和解码器的隐藏单元个数均为$h$，且函数$a(\\boldsymbol{s}, \\boldsymbol{h})=\\boldsymbol{s}^\\top \\boldsymbol{h}$。假设我们希望根据解码器单个隐藏状态$\\boldsymbol{s}_{t' - 1} \\in \\mathbb{R}^{h}$和编码器所有隐藏状态$\\boldsymbol{h}_t \\in \\mathbb{R}^{h}, t = 1,\\ldots,T$来计算背景向量$\\boldsymbol{c}_{t'}\\in \\mathbb{R}^{h}$。\n",
    "我们可以将查询项矩阵$\\boldsymbol{Q} \\in \\mathbb{R}^{1 \\times h}$设为$\\boldsymbol{s}_{t' - 1}^\\top$，并令键项矩阵$\\boldsymbol{K} \\in \\mathbb{R}^{T \\times h}$和值项矩阵$\\boldsymbol{V} \\in \\mathbb{R}^{T \\times h}$相同且第$t$行均为$\\boldsymbol{h}_t^\\top$。此时，我们只需要通过矢量化计算\n",
    "\n",
    "$$\\text{softmax}(\\boldsymbol{Q}\\boldsymbol{K}^\\top)\\boldsymbol{V}$$\n",
    "\n",
    "即可算出转置后的背景向量$\\boldsymbol{c}_{t'}^\\top$。当查询项矩阵$\\boldsymbol{Q}$的行数为$n$时，上式将得到$n$行的输出矩阵。输出矩阵与查询项矩阵在相同行上一一对应。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2 更新隐藏状态\n",
    "\n",
    "现在我们描述第二个关键点，即更新隐藏状态。以门控循环单元为例，在解码器中我们可以对GRU中的门控循环单元的设计稍作修改，从而变换上一时间步$t'-1$的输出$\\boldsymbol{y}_{t'-1}$、隐藏状态$\\boldsymbol{s}_{t' - 1}$和当前时间步$t'$的含注意力机制的背景变量$\\boldsymbol{c}_{t'}$。解码器在时间步$t'$的隐藏状态为\n",
    "\n",
    "$$\\boldsymbol{s}_{t'} = \\boldsymbol{z}_{t'} \\odot \\boldsymbol{s}_{t'-1}  + (1 - \\boldsymbol{z}_{t'}) \\odot \\tilde{\\boldsymbol{s}}_{t'},$$\n",
    "\n",
    "其中的重置门、更新门和候选隐藏状态分别为\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\boldsymbol{r}_{t'} &= \\sigma(\\boldsymbol{W}_{yr} \\boldsymbol{y}_{t'-1} + \\boldsymbol{W}_{sr} \\boldsymbol{s}_{t' - 1} + \\boldsymbol{W}_{cr} \\boldsymbol{c}_{t'} + \\boldsymbol{b}_r),\\\\\n",
    "\\boldsymbol{z}_{t'} &= \\sigma(\\boldsymbol{W}_{yz} \\boldsymbol{y}_{t'-1} + \\boldsymbol{W}_{sz} \\boldsymbol{s}_{t' - 1} + \\boldsymbol{W}_{cz} \\boldsymbol{c}_{t'} + \\boldsymbol{b}_z),\\\\\n",
    "\\tilde{\\boldsymbol{s}}_{t'} &= \\text{tanh}(\\boldsymbol{W}_{ys} \\boldsymbol{y}_{t'-1} + \\boldsymbol{W}_{ss} (\\boldsymbol{s}_{t' - 1} \\odot \\boldsymbol{r}_{t'}) + \\boldsymbol{W}_{cs} \\boldsymbol{c}_{t'} + \\boldsymbol{b}_s),\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "其中含下标的$\\boldsymbol{W}$和$\\boldsymbol{b}$分别为门控循环单元的权重参数和偏差参数。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**总结**：\n",
    "\n",
    "本质上，注意力机制能够为编码信息中较有价值的部分分配较多的计算资源。这个有趣的想法自提出后得到了快速发展，特别是启发了依靠注意力机制来编码输入序列并解码出输出序列的Transformer模型的设计。Transformer抛弃了卷积神经网络和循环神经网络的架构，在计算效率上比基于循环神经网络的编码器—解码器模型通常更具明显优势。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
