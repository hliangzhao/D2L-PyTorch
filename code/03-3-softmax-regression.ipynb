{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 softmax回归的理论模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 模型定义\n",
    "本章节中，如无特别说明，向量均指列向量。\n",
    "\n",
    "softmax回归是logistic回归（适用于2类分类）扩展到多类分类的结果。设标签$c \\in \\{1, ..., C \\}$，对于样本$(\\vec{x}, y)$，\n",
    "softmax回归预测样本标签为$c$的概率为\n",
    "$$\n",
    "p(y=c|\\vec{x}) = softmax(\\vec{w}_c^\\top \\vec{x}) = \\frac{\\exp (\\vec{w}_c^\\top \\vec{x})}{\\sum_{c'=1}^C \\exp (\\vec{w}_{c'}^\\top \\vec{x})},\n",
    "$$\n",
    "因此softmax回归的预测结果为\n",
    "$$\n",
    "\\hat{y} = argmax_{c=1}^C p(y=c|\\vec{x}) = argmax_{c=1}^C \\vec{w}_c^\\top \\vec{x}.\n",
    "$$\n",
    "本质上，softmax回归是一个单层神经网络，输出层为softmax层。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 单样本的矢量计算表达式\n",
    "为了更方便地定义torch tensor，假设所有向量均为行向量。\n",
    "\n",
    "设$d$为样本特征个数且$\\vec{x} \\in \\mathbb{R}^{1 \\times d}$，$W \\in \\mathbb{R}^{d \\times C}$为待学习的权重，$\\vec{b} \\in \\mathbb{R}^{1 \\times C}$为偏置，则对于样本$(\\vec{x}^{(i)}, y^{(i)})$，softmax回归的矢量计算表达式为\n",
    "$$\n",
    "\\hat{\\vec{y}}^{(i)} = softmax (\\vec{x}^{(i)} W + \\vec{b}),\n",
    "$$\n",
    "其中$\\hat{\\vec{y}}^{(i)}$的各个元素反应了softmax回归预测各标签的概率。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 多样本的矢量计算表达式"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "为了更方便地定义torch tensor，假设所有向量均为行向量。\n",
    "\n",
    "令$X \\in \\mathbb{R}^{n \\times d}$是$n$个样本的特征矩阵，则\n",
    "$$\n",
    "\\hat{Y} = softmax (XW+b).\n",
    "$$\n",
    "PyTorch会自动开启广播模式。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 参数学习\n",
    "采用交叉熵损失函数，只关心正确类别的预测概率：\n",
    "$$\n",
    "l (W, \\vec{b}) = - \\frac{1}{N} \\sum_{n=1}^N \\sum_{c=1}^C y_c^{(n)} \\log \\hat{y}_c^{(n)}.\n",
    "$$\n",
    "其梯度的求导过程并不难，虽然略复杂。最终参数的更新公式为\n",
    "$$\n",
    "W_{t+1} \\leftarrow W_t + \\alpha \\bigg( \\frac{1}{|\\mathcal{B}|} \\sum_{n \\in \\mathcal{B}} \\vec{x}^{(n)} \\Big(\\vec{y}^{(n)} - \\hat{\\vec{y}}^{(n)}_{W_t}\\Big)^\\top \\bigg)\n",
    "$$\n",
    "$$\n",
    "b_{t+1} \\leftarrow b_t + \\alpha \\bigg( \\frac{1}{|\\mathcal{B}|} \\sum_{n \\in \\mathcal{B}} \\Big(\\vec{y}^{(n)} - \\hat{\\vec{y}}^{(n)}_{b_t}\\Big) \\bigg)\n",
    "$$\n",
    "其中$\\vec{y}^{(n)}$是一个向量，仅有true label位置对应的元素为1。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
