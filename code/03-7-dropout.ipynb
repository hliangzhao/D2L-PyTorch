{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 Dropout：理论模型\n",
    "在讲解MLP时我们给出了如下图所示的带有隐藏层的神经网络：\n",
    "![avatar](../resource/mlp.svg)\n",
    "其中，对于单个样本$\\big([x_1, ..., x_4]^\\top, y\\big)$，隐藏单元$h_i$的计算表达式为\n",
    "$$\n",
    "h_i = \\phi \\Big(\\vec{x}^\\top W_h(:,i) + b_h(i)\\Big)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "若对该隐藏层使用dropout，则该层的每个隐藏单元有一定概率会被丢弃掉。设丢弃概率（超参数）为$p$，则$\\forall i, h_i$有$p$的概率会被清零，有$1-p$的概率会被做拉伸。用数学语言描述即\n",
    "$$\n",
    "h'_i = \\frac{\\xi_i}{1-p}h_i,\n",
    "$$\n",
    "其中$\\xi_i$是一个随机变量，$p(\\xi_i = 0) = p$，$p(\\xi_i=1) = 1-p$。\n",
    "则\n",
    "$$\n",
    "\\mathbb{E} [h'_i] = h_i.\n",
    "$$\n",
    "这意味着**dropout不改变输入的期望输出**（这就是要除以$1-p$的原因）。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "对上述MLP训练的时候使用dropout，一种可能的网络结构如下：\n",
    "![avatar](../resource/dropout.svg)\n",
    "此时MLP的输出不依赖$h_2$和$h_5$。由于在训练中隐藏层神经元的丢弃是随机的，即$h_1, ..., h_5$都有可能被清零，输出层的计算无法过度依赖$h_1, ..., h_5$中的任一个，从而在训练模型时起到正则化的作用，并可以用来应对过拟合。\n",
    "\n",
    "**Dropout是一种训练时应对过拟合的方法，并未改变网络的结构。当参数训练完毕并用于测试时，任何参数都不会被dropout。**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 Dropout：从零开始实现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.],\n",
      "        [ 8.,  9., 10., 11., 12., 13., 14., 15.]])\n",
      "tensor([[ 0.,  0.,  0.,  0.,  0., 10.,  0.,  0.],\n",
      "        [ 0., 18., 20., 22., 24.,  0., 28., 30.]])\n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0.]])\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import my_utils\n",
    "\n",
    "def dropout(X, drop_prob):\n",
    "    X = X.float()\n",
    "    assert 0 <= drop_prob <= 1\n",
    "    keep_prob = 1 - drop_prob\n",
    "    if keep_prob == 0:\n",
    "        return torch.zeros_like(X)\n",
    "    # mask = 0则对应位置的x将会被dropout，注意要除以keep_prob！\n",
    "    mask = (torch.rand(X.shape) < keep_prob).float()\n",
    "    return mask * X / keep_prob\n",
    "\n",
    "# 测试\n",
    "X = torch.arange(16).view(2, 8)\n",
    "print(dropout(X, 0))\n",
    "print(dropout(X, 0.5))\n",
    "print(dropout(X, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 定义含有两个隐藏层的MLP模型参数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "使用Fashion-MNIST数据集进行训练和预测："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_inputs, num_outputs, num_hiddens1, num_hiddens2 = 784, 10, 256, 256\n",
    "W1 = torch.tensor(\n",
    "    np.random.normal(0, 0.01, size=(num_inputs, num_hiddens1)), \n",
    "    dtype=torch.float, requires_grad=True)\n",
    "b1 = torch.zeros(num_hiddens1, requires_grad=True)\n",
    "W2 = torch.tensor(\n",
    "    np.random.normal(0, 0.01, size=(num_hiddens1, num_hiddens2)),\n",
    "    dtype=torch.float, requires_grad=True)\n",
    "b2 = torch.zeros(num_hiddens2, requires_grad=True)\n",
    "W3 = torch.tensor(\n",
    "    np.random.normal(0, 0.01, size=(num_hiddens2, num_outputs)),\n",
    "    dtype=torch.float, requires_grad=True)\n",
    "b3 = torch.zeros(num_outputs, requires_grad=True)\n",
    "\n",
    "params = [W1, b1, W2, b2, W3, b3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 定义模型（在两个隐藏层上使用dropout）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_prob1, drop_prob2 = 0.2, 0.5\n",
    "\n",
    "def net(X, is_training=True):\n",
    "    X = X.view(-1, num_inputs)\n",
    "    H1 = (torch.matmul(X, W1) + b1).relu()\n",
    "    if is_training:\n",
    "        H1 = dropout(H1, drop_prob1)\n",
    "    H2 = (torch.matmul(H1, W2) + b2).relu()\n",
    "    if is_training:\n",
    "        H2 = dropout(H2, drop_prob2)\n",
    "    return torch.matmul(H2, W3) + b3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 定义模型评估函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_accuracy(data_iter, net):\n",
    "    acc_sum, n = 0.0, 0\n",
    "    for X, y in data_iter:\n",
    "        if isinstance(net, torch.nn.Module):\n",
    "            # 如果模型是通过torch来定义的（必然是nn.Module），\n",
    "            # 则torch会根据net当前是处于评估模式（eval）还是训练模式（train）来进行抉择是否dropout\n",
    "            # 评估模型自然要进入eval模式，但是别忘了评估结束后切换回来\n",
    "            net.eval()\n",
    "            acc_sum += (net(X).argmax(dim=1) == y).float().sum().item()\n",
    "            net.train()\n",
    "        else:\n",
    "            # 模型时从零开始实现的\n",
    "            if('is_training' in net.__code__.co_varnames):\n",
    "                # 若模型使用了dropout，则要将is_training设置为false\n",
    "                acc_sum += (net(X, is_training=False).argmax(dim=1) == y).float().sum().item()\n",
    "            else:\n",
    "                acc_sum += (net(X).argmax(dim=1) == y).float().sum().item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 训练并测试模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1, loss 0.0047, train acc 0.541, test acc 0.721\n",
      "epoch 2, loss 0.0023, train acc 0.783, test acc 0.742\n",
      "epoch 3, loss 0.0019, train acc 0.820, test acc 0.772\n",
      "epoch 4, loss 0.0018, train acc 0.837, test acc 0.824\n",
      "epoch 5, loss 0.0016, train acc 0.848, test acc 0.844\n"
     ]
    }
   ],
   "source": [
    "num_epochs, lr, batch_size = 5, 100., 256    # 注意这里的学习旅需要设置得很大\n",
    "loss = torch.nn.CrossEntropyLoss()\n",
    "train_iter, test_iter = my_utils.load_fashion_mnist(batch_size)\n",
    "my_utils.general_train(net, train_iter, test_iter, loss, num_epochs, batch_size, params, lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 Dropout：PyTorch实现（直接使用torch.nn.dropout()）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 定义模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = nn.Sequential(\n",
    "    my_utils.FlattenLayer(),\n",
    "    nn.Linear(num_inputs, num_hiddens1),\n",
    "    nn.ReLU(),\n",
    "    nn.Dropout(drop_prob1),\n",
    "    nn.Linear(num_hiddens1, num_hiddens2),\n",
    "    nn.ReLU(),\n",
    "    nn.Dropout(drop_prob2),\n",
    "    nn.Linear(num_hiddens2, 10))\n",
    "\n",
    "for params in net.parameters():\n",
    "    nn.init.normal_(params, mean=0, std=0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 训练并测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1, loss 0.0052, train acc 0.495, test acc 0.681\n",
      "epoch 2, loss 0.0024, train acc 0.768, test acc 0.799\n",
      "epoch 3, loss 0.0021, train acc 0.809, test acc 0.805\n",
      "epoch 4, loss 0.0018, train acc 0.829, test acc 0.825\n",
      "epoch 5, loss 0.0017, train acc 0.841, test acc 0.831\n"
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.SGD(net.parameters(), lr=0.5)\n",
    "my_utils.general_train(net, train_iter, test_iter, loss, num_epochs, batch_size, None, None, optimizer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
